{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> MACHINE LEARNING - PROJECT </center></h1>\n",
    "<center> \"WHAT ARE THE PEOPLE MORE LIKELY TO SURVIVE TO THE BOOLEAN PANDEMIC?\"</center>\n",
    "\n",
    "Notebook structure:\n",
    "* [1. Sample](#sample)\n",
    "    * [1.1. Import Libraries](#import)\n",
    "    * [1.2. Import Datasets](#import2)\n",
    "* [2. Explore](#explore)\n",
    "    * [2.1. Data Exploration](#dataexplore)\n",
    "    * [2.2. Missing Values Analysis](#miss_values)\n",
    "    * [2.3. Outliers Analysis](#outliers)\n",
    "* [3. Modify](#modify)\n",
    "    * [3.1. Transform and Create variables](#transf_create)\n",
    "    * [3.2. Coherence Checking](#coherence)\n",
    "    * [3.3. Correlation analysis](#corr)\n",
    "    * [3.4. Train Validation Partition](#train_val)\n",
    "    * [3.5. Data Standardization](#datastand)\n",
    "    * [3.6. Feature Selection](#feature)\n",
    "* [4. Model](#model)\n",
    "    * [4.1. K Nearest Neighbors](#knn)\n",
    "    * [4.2. K Nearest Centroid](#knc)\n",
    "    * [4.3. Random Forest](#rf)\n",
    "    * [4.4. Decision Tree](#dt)\n",
    "    * [4.5. Passive Aggressive](#pa)\n",
    "    * [4.6. Multi-Layer Perceptron](#mlp)\n",
    "\n",
    "* [5. Assess](#assess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"sample\">\n",
    "    \n",
    "# 1. Sample\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import\">\n",
    "\n",
    "## 1.1. Import Libraries\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from itertools import combinations_with_replacement\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RidgeCV, PassiveAggressiveClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"import2\">\n",
    "\n",
    "## 1.2. Import Datasets\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'Data/train.csv')\n",
    "test_df = pd.read_csv(r'Data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"explore\">\n",
    "    \n",
    "# 2. Explore\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"dataexplore\">\n",
    "\n",
    "## 2.1. Data Exploration\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Severity', 'Birthday_year', 'Parents or siblings infected', 'Wife/Husband or children infected', \n",
    "    'Medical_Expenses_Family', 'Deceased']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Deceased'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOTE:` Unbalanced learning, test over/under sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"miss_values\">\n",
    "\n",
    "## 2.2. Missing Values Analysis\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of missing values by variable:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# of missing values by variable:\")\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Medical Tent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the variable \"Medical Tent\", once it as 702 missing values from a total of 900 (78%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='Medical_Tent')\n",
    "test_df = test_df.drop(columns='Medical_Tent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To fill the missing values in the variable \"City\", we decide to use the mode, since there are only to observations missing city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.City.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['City'] = df['City'].fillna(df['City'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.City.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Birthday Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what concerns the remaining missing values, all from the variable \"Birthday Year\", we decided to apply the K-Nearest-Neighbor algorithm to fill them. This decision was based in the fact that there are 177 missing values, which we consider too much for apllying a simple input (such as mean or median input), but not that many too remove a variable that we consider that might have some importance in our model. \n",
    "Later, with more knowledge of the dataset we might consider remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "knn_vars = df.drop(['Patient_ID', 'Name', 'City', 'Deceased'], axis = 1)\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "X_filled_knn = imputer.fit_transform(knn_vars)\n",
    "years = np.round(X_filled_knn[:,2])\n",
    "\n",
    "for i in range(len(knn_vars)):\n",
    "    if knn_vars.loc[i,'Birthday_year'] < 1900:\n",
    "        print (years[i])\n",
    "        \n",
    "df['Birthday_year'] = years\n",
    "\n",
    "# Test set\n",
    "knn_vars_test = test_df.drop(['Patient_ID', 'Name', 'City'], axis = 1)\n",
    "imputer = KNNImputer(n_neighbors=10)\n",
    "Xtest_filled_knn = imputer.fit_transform(knn_vars_test)\n",
    "years_df = np.round(Xtest_filled_knn[:,2])\n",
    "test_df['Birthday_year'] = years_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"outliers\">\n",
    "\n",
    "## 2.3. Outliers Analysis\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,2, figsize=(10, 5), squeeze=False)    \n",
    "sns.boxplot(df[\"Birthday_year\"], color=\"skyblue\", ax=axes[0, 0])\n",
    "sns.boxplot(df[\"Medical_Expenses_Family\"], color=\"blue\", ax=axes[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "sns.distplot(df[\"Birthday_year\"], color=\"skyblue\", ax=axes[0, 0], kde=False)\n",
    "sns.distplot(df[\"Parents or siblings infected\"], color=\"steelblue\", ax=axes[0, 1], kde=False)\n",
    "sns.distplot(df[\"Medical_Expenses_Family\"], color=\"blue\", ax=axes[1, 0], kde=False)\n",
    "sns.distplot(df[\"Wife/Husband or children infected\"], color=\"c\", ax=axes[1, 1], kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Outlier'] = 0\n",
    "df.loc[df['Medical_Expenses_Family']>13000, 'Outlier']=1\n",
    "df['Outlier'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['Outlier'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"modify\">\n",
    "\n",
    "# 3. Modify\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"transf_create\">\n",
    "\n",
    "## 3.1. Transform and Create variables\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['City'] == 'Santa Fe', 'Santa Fe'] = 1\n",
    "df.loc[df['City'] != 'Santa Fe', 'Santa Fe'] = 0\n",
    "test_df.loc[test_df['City'] == 'Santa Fe', 'Santa Fe'] = 1\n",
    "test_df.loc[test_df['City'] != 'Santa Fe', 'Santa Fe'] = 0\n",
    "\n",
    "df.loc[df['City'] == 'Taos', 'Taos'] = 1\n",
    "df.loc[df['City'] != 'Taos', 'Taos'] = 0\n",
    "test_df.loc[test_df['City'] == 'Taos', 'Taos'] = 1\n",
    "test_df.loc[test_df['City'] != 'Taos', 'Taos'] = 0\n",
    "\n",
    "df['Age'] = 2020 - df['Birthday_year']\n",
    "test_df['Age'] = 2020 - test_df['Birthday_year']\n",
    "\n",
    "df['Family_cases'] = df['Parents or siblings infected'] + df['Wife/Husband or children infected']\n",
    "test_df['Family_cases'] = test_df['Parents or siblings infected'] + test_df['Wife/Husband or children infected']\n",
    "\n",
    "Family_size = pd.DataFrame(df['Patient_ID'].groupby(df['Family_Case_ID']).count())\n",
    "Family_size = Family_size.rename({'Patient_ID':'Family_size'}, axis='columns') \n",
    "df = df.merge(Family_size, on = ['Family_Case_ID'])\n",
    "\n",
    "Family_size_test = pd.DataFrame(test_df['Patient_ID'].groupby(test_df['Family_Case_ID']).count())\n",
    "Family_size_test = Family_size_test.rename({'Patient_ID':'Family_size'}, axis='columns') \n",
    "test_df = test_df.merge(Family_size_test, on = ['Family_Case_ID'])\n",
    "\n",
    "df['Medical_Expenses_Person'] = df['Medical_Expenses_Family']/df['Family_size']\n",
    "test_df['Medical_Expenses_Person'] = test_df['Medical_Expenses_Family']/test_df['Family_size']\n",
    "\n",
    "df['Parents_infected_bin'] = 0\n",
    "df.loc[df['Parents or siblings infected'] != 0, 'Parents_infected_bin'] = 1\n",
    "test_df['Parents_infected_bin'] = 0\n",
    "test_df.loc[test_df['Parents or siblings infected'] != 0, 'Parents_infected_bin'] = 1\n",
    "\n",
    "df['WifeOrChildren_infected_bin'] = 0\n",
    "df.loc[df['Wife/Husband or children infected'] != 0, 'WifeOrChildren_infected_bin'] = 1\n",
    "test_df['WifeOrChildren_infected_bin'] = 0\n",
    "test_df.loc[test_df['Wife/Husband or children infected'] != 0, 'WifeOrChildren_infected_bin'] = 1\n",
    "\n",
    "df.drop(columns = ['City','Name','Outlier','Family_Case_ID','Birthday_year'], inplace = True)\n",
    "test_df.drop(columns = ['City','Name','Family_Case_ID','Birthday_year'], inplace = True)\n",
    "\n",
    "df.set_index('Patient_ID', inplace = True)\n",
    "test_df.set_index('Patient_ID', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family_cases/family_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"coherence\">\n",
    "\n",
    "## 3.2. Coherence Checking\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2['Incoherent'] = 0\n",
    "#Acho que deviamos tirar isto, não faz sentdo considerarmos incoerencias e deixarmos ficar\n",
    "df2.loc[df2['Family_cases'] > df2['Family_size'], 'Incoherent'] = 1\n",
    "df2.loc[(df['Age'] > 120) | (df2['Age'] < 0), 'Incoherent'] = 1\n",
    "df2['Incoherent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"corr\">\n",
    "\n",
    "## 3.3. Correlation Analysis\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmar se incluímos a dependente na matriz de correlação\n",
    "plt.rcParams['figure.figsize'] = (12,12)\n",
    "\n",
    "corr_matrix=df.drop(columns=['Wife/Husband or children infected', 'Parents or siblings infected']).corr(method = 'spearman')\n",
    "mask=np.zeros_like(corr_matrix, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "sns.heatmap(data=corr_matrix, mask=mask, center=0, annot=True, linewidths=2, cmap='coolwarm')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation analysis and based on a threshold of 0.8 (or -0.8) we can conclude that we should choose between removing the variable \"Family Cases\" or both the variables \"Parents or Siblings Infected\" and \"Wife/Husband or children infected\". Besides that, there are some values that we should pay some attention, specially the ones related with the variable \"Medical_Expenses_Person\", which also seems to be strongly correlated with the \"Medical expenses_family\" (0.75) and the \"Severaty\" (-0.79).\n",
    "\n",
    "In section 3.5., we will procceed to feature selection where we will take into account the values obtained with this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"train_val\">\n",
    "\n",
    "## 3.4. Train Validation Partition\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Severity', 'Age', 'Santa Fe', 'Taos', 'Parents_infected_bin', 'WifeOrChildren_infected_bin',\n",
    "             'Family_cases', 'Family_size', 'Medical_Expenses_Person']\n",
    "\n",
    "X = df[variables]\n",
    "\n",
    "y = df['Deceased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, \n",
    "                                                  random_state=15, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"datastand\">\n",
    "\n",
    "## 3.5. Standardization\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)  # z-score\n",
    "#scaler = RobustScaler().fit(X_train) # robust standardization\n",
    "\n",
    "scaler_X_train = scaler.transform(X_train)\n",
    "scaler_X_train = pd.DataFrame(scaler_X_train, columns=variables)\n",
    "\n",
    "scaler_X_val = scaler.transform(X_val)\n",
    "scaler_X_val = pd.DataFrame(scaler_X_val, columns=variables)\n",
    "\n",
    "test_df = test_df[variables]\n",
    "\n",
    "scaler_X_test = scaler.transform(test_df)\n",
    "scaler_X_test = pd.DataFrame(scaler_X_test, columns=variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max normalization\n",
    "#minmax = MinMaxScaler(feature_range=(-1,1)).fit(X_train)\n",
    "##minmax = MinMaxScaler().fit(X_train)\n",
    "#scaler_X_train = minmax.transform(X_train)\n",
    "#scaler_X_val = minmax.transform(X_val)\n",
    "#scaler_X_test = minmax.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"feature\">\n",
    "\n",
    "## 3.6. Feature Selection\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"lasso\">\n",
    "\n",
    "### 3.6.1. Lasso Regression\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(coef,name):\n",
    "    imp_coef = coef.sort_values()\n",
    "    plt.figure(figsize=(8,10))\n",
    "    imp_coef.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LassoCV()\n",
    "reg.fit(scaler_X_train, y_train)\n",
    "coef = pd.Series(reg.coef_, index=scaler_X_train.columns)\n",
    "coef.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(coef, 'Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ridge\">\n",
    "\n",
    "### 3.6.2. Ridge Regression\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV()\n",
    "ridge.fit(X=scaler_X_train, y=y_train)\n",
    "coef_ridge = pd.Series(ridge.coef_, index=scaler_X_train.columns)\n",
    "print(coef_ridge.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(coef_ridge,'Ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"rfe\">\n",
    "\n",
    "### 3.6.3. Recursive Feature Elimination (RFE)\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of features\n",
    "nof_list=np.arange(1,10)            \n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "\n",
    "    X_train_rfe, X_rfe_val, y_train_rfe, y_rfe_val = train_test_split(scaler_X_train, y_train, test_size = 0.2, \n",
    "                                                                      random_state = 100)\n",
    "    \n",
    "    model_rfe = LogisticRegression()\n",
    "    rfe = RFE(model_rfe,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train_rfe,y_train_rfe)\n",
    "    X_rfe_val = rfe.transform(X_rfe_val)\n",
    "    \n",
    "    model_rfe.fit(X_train_rfe,y_train_rfe)\n",
    "    \n",
    "    score = model_rfe.score(X_rfe_val,y_rfe_val)\n",
    "    score_list.append(score)\n",
    "    \n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "        \n",
    "\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6\n",
    "model_rfe = LogisticRegression()\n",
    "rfe = RFE(estimator = model_rfe, n_features_to_select = N)\n",
    "X_rfe = rfe.fit_transform(X = scaler_X_train, y = y_train) \n",
    "\n",
    "selected_features_rfe = pd.Series(rfe.ranking_, index = scaler_X_train.columns)\n",
    "selected_features_rfe.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"model\">\n",
    "\n",
    "# 4. Model\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"knn\">\n",
    "\n",
    "## 4.1. K Nearest Neighbors\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "knn_parameters = {'n_neighbors' : np.arange(13,31,1),\n",
    "                  'metric' : ['euclidean', 'cosine', 'manhattan', 'minkowski'],\n",
    "                  'weights' : ['uniform', 'distance'],\n",
    "                  'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "knn_grid = GridSearchCV(estimator=knn_clf, param_grid=knn_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "knn_grid.fit(scaler_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"knc\">\n",
    "\n",
    "## 4.2. K Nearest Centroid\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc_clf = NearestCentroid()\n",
    "\n",
    "knc_parameters = {'metric' : ['euclidean', 'cosine', 'manhattan']}\n",
    "\n",
    "knc_grid = GridSearchCV(estimator=knc_clf, param_grid=knc_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "knc_grid.fit(scaler_X_train, y_train)\n",
    "knc_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"rfc\">\n",
    "\n",
    "## 4.3. Random Forest\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(class_weight='balanced', random_state=15)\n",
    "\n",
    "rf_parameters = {\"n_estimators\": np.arange(100, 400, 100),\n",
    "                 \"max_features\": ['sqrt', 'log2', 'auto', None],\n",
    "                 \"criterion\": ['gini', 'entropy'],\n",
    "                 \"warm_start\" : [True, False]}\n",
    "\n",
    "rf_grid = GridSearchCV(estimator=rf_clf, param_grid=rf_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"dt\">\n",
    "\n",
    "## 4.4. Decision Tree\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(class_weight='balanced', random_state=15)\n",
    "\n",
    "dt_parameters = {\"max_features\": ['sqrt', 'log2', 'auto', None],\n",
    "                 \"splitter\" : ['best', 'random'],\n",
    "                 \"criterion\": ['gini', 'entropy']}\n",
    "\n",
    "dt_grid = GridSearchCV(estimator=dt_clf, param_grid=dt_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "dt_grid.fit(X_train, y_train)\n",
    "dt_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"pa\">\n",
    "\n",
    "## 4.5. Passive Aggressive\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_clf = PassiveAggressiveClassifier(class_weight='balanced', random_state=15)\n",
    "\n",
    "pa_parameters = {\"warm_start\" : [True, False],\n",
    "                 \"early_stopping\" : [True, False],\n",
    "                 \"max_iter\" : (100, 500, 1000)}\n",
    "\n",
    "pa_grid = GridSearchCV(estimator=pa_clf, param_grid=pa_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "pa_grid.fit(scaler_X_train, y_train)\n",
    "pa_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"logreg\">\n",
    "\n",
    "## 4.6. Logistic Regression\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(multi_class='multinomial')\n",
    "\n",
    "log_parameters = {'solver': ['newton-cg', 'sag', 'saga', 'lbfgs'],\n",
    "                  'warm_start' : [True, False],\n",
    "                  'max_iter' : (100, 200, 300)}\n",
    "\n",
    "log_grid = GridSearchCV(estimator=log_clf, param_grid=log_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "log_grid.fit(scaler_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"mlp\">\n",
    "\n",
    "## 4.7. Multi-Layer Perceptron\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid 1\n",
    "###### Note: Definir os params para um n limitado de Neurons/ Hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=15, max_iter=600)\n",
    "\n",
    "mlp_parameters = {\n",
    "#    'hidden_layer_sizes': [(50,),(50,50,),(50,50,50)],\n",
    "    'activation': ['identity','logistic','tanh', 'relu'],\n",
    "    'solver': ['lbfgs','sgd', 'adam'],\n",
    "    'alpha':np.logspace(-5, 3, 5),\n",
    "#     'batch_size':(),\n",
    "    'learning_rate_init': list(np.linspace(0.00001,0.1,5)),\n",
    "    'warm_start': [True,False],\n",
    "    'learning_rate': ['constant','invscaling','adaptive'],\n",
    "    'early_stopping' : [True,False]\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(estimator=mlp, param_grid=mlp_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "mlp_grid.fit(scaler_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid 2.\n",
    "###### Note: Para os params acima encontrados testar todas as combinações de layers/neurons possiveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination_layers(min_neurons,max_neurons,n_layers): \n",
    "    l = []\n",
    "    for i in range(min_neurons,max_neurons):\n",
    "        l.append(i)\n",
    "    layersize = list(combinations_with_replacement(l,n_layers))\n",
    "    return layersize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=15, max_iter=600, activation='tanh', solver='lbfgs', alpha=10, \n",
    "                    learning_rate='constant', learning_rate_init=1e-05, warm_start=True, early_stopping=True)\n",
    "\n",
    "mlp_parameters = {\n",
    "    'hidden_layer_sizes': combination_layers(10,50,2)\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(estimator=mlp, param_grid=mlp_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "mlp_grid.fit(scaler_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"ensemble\">\n",
    "\n",
    "## 4.8. Ensemble \n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to enhance the performance of the model, ensemble was implemented. The main goal is to provide the ability to combine conceptually different machine learning classifiers into one single classifier, boosting their individual performance and tackling their individual weaknesses, while reducing existing overfitting. There are two different approaches to reach this goal, this is, bagging and boosting. Bagging uses complex base models and tries to \"smooth out\" their predictions while Boosting expects that individual mediocre models can be combined to create a high-performance model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"bbc\">\n",
    "\n",
    "### 4.8.1. Balanced Bagging Classifier\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_clf = BalancedBaggingClassifier(base_estimator=[mlp_grid, log_grid, rf_grid, knn_grid])\n",
    "\n",
    "bbc_parameters = {\n",
    "    #'base_estimator' : [None, [mlp_grid, log_grid, rf_grid, knn_grid]],\n",
    "                  'n_estimators' : (5, 10, 15, 20),\n",
    "                  'bootstrap' : [True, False],\n",
    "                  'bootstrap_features' : [True, False],\n",
    "                  'warm_start' : [True, False],\n",
    "                  'sampling_strategy' : ['not majority', 'not minority', 'all', 'majority', 'minority'],\n",
    "                  'replacement' : [True, False]}\n",
    "\n",
    "bbc_grid = GridSearchCV(estimator=bbc_clf, param_grid=bbc_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "bbc_grid.fit(scaler_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"gbc\">\n",
    "\n",
    "### 4.8.2. Gradient Boosting Classifier\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_clf = GradientBoostingClassifier()\n",
    "\n",
    "GB_parameters = {'loss' : ['deviance', 'exponential'],\n",
    "                 'learning_rate' : (0.01, 0.1, 1),\n",
    "                 'n_estimators' : np.arange(100, 400, 100),\n",
    "                 'max_depth' : (5, 10, 15, 20, 30),\n",
    "                 'max_features' : ['auto', 'log2', None],\n",
    "                 'warm_start' : [True, False]}\n",
    "\n",
    "GB_grid = GridSearchCV(estimator=GB_clf, param_grid=GB_parameters, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "GB_grid.fit(scaler_X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"adaboost\">\n",
    "\n",
    "### 4.8.3. AdaBoost Classifier\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost = AdaBoostClassifier()\n",
    "\n",
    "AdaBoost_parameters = {'base_estimator' : [None, [knn_grid, log_grid], knn_grid, GB_grid],\n",
    "                       'n_estimators' : np.arange(50, 200, 50),\n",
    "                       'learning_rate' : (0.01, 0.1, 1, 10)}\n",
    "\n",
    "AdaBoost_grid = GridSearchCV(estimator=AdaBoost, param_grid=AdaBoost_parameters, cv=cv, \n",
    "                             scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "AdaBoost_grid.fit(scaler_X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"vc\">\n",
    "\n",
    "### 4.8.4. Voting Classifier\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=[('lr', log_grid), ('knn', knn_grid), ('gnb', nb_clf), \n",
    "                                  ('gb', GB_grid), ('adaboost', AdaBoost_grid)]).fit(scaler_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a class=\"anchor\" id=\"assess\">\n",
    "\n",
    "# 5. Assess\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = knn_grid\n",
    "accuracy = accuracy_score(y_val, labels_val)\n",
    "labels_train = model.predict(scaler_X_train)\n",
    "labels_val = model.predict(scaler_X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_train, pred_train , y_val, pred_val):\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                     TRAIN                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_train, pred_train))\n",
    "    print(confusion_matrix(y_train, pred_train))\n",
    "\n",
    "\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                VALIDATION                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_val, pred_val))\n",
    "    print(confusion_matrix(y_val, pred_val))\n",
    "    \n",
    "metrics(y_train, labels_train, y_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters (variables, k, model, accuracy):\n",
    "    print(\"Variables:\", variables )\n",
    "    print(\"Nº of folds:\", k)\n",
    "    print(\"Parameters:\", model.best_params_)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print_parameters(variables, k, model, accuracy = accuracy_score(y_val, labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 3\n",
    "\n",
    "labels_test = model.predict(scaler_X_test)\n",
    "test_output = pd.DataFrame(labels_test, columns = [\"Deceased\"])\n",
    "test_output[\"Patient_ID\"] = test_df.index\n",
    "cols = list(test_output)\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "test_output = test_output[cols]\n",
    "test_output = test_output.sort_values(by=['Patient_ID'])\n",
    "\n",
    "path = '.\\\\Data\\\\Group38_version' + str(version) + '.csv'\n",
    "test_output.to_csv(path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
